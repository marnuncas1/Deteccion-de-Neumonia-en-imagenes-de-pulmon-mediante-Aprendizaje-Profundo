{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprenderemos como usamos para emitir un descenso de gradiente para establecer los valores numericos en capas densas y convolucionales asi tambien como argumentos como el tamaño del lote y el numero de epocas afectan la capacitacion de este modelo.\n",
    "\n",
    "Comenzaremos con un ejemplo usando capas densas:\n",
    "Cada entrada esta conectada a las posibles salidas, segun se corresponde con capas densas, cada conexion tendrá un valor numerico llamado peso. En un primer proceso de una persona completamos los datos de entrada y podemos calcular los valores en las proximas capas usando un proceso llamado propagacion directa.\n",
    "El nodo de la capa de salida tiene 3 conexiones (nº de entradas) , multiplicamos el peso V por el valor en el nodo de donde proviene la conexion, por ejemplo \n",
    "Entradas:\n",
    "- Edad = 50 con un peso de 0.1\n",
    "- Peso = 80 con un peso de 0.1\n",
    "- Azucar en sangre = 80 con un peso de -0.1 \n",
    "Para la salida **si tiene diabetes** el resultado será 0.1(50)+ 0.1(80)-0.1(80)= 5\n",
    "\n",
    "Realizamos ese calculo para las posibles salidas. Posteriormente la función softmax convierte los valores en prueba de probabilidades. Flow maneja los calculos para que no necesite recordar el softmax. La funcion proporciona un valor de 5% para el nodo superior (si diabetes) y un 95% para el inferior, por lo que la red predijo que esta persona con esos datos de entrada tiene un 5% de probabilidades de desarrollar diabetes y un 95% de no desarrollarla.\n",
    "\n",
    "Cuando se trabaja con datos tabulares, es común tener muchas capas densas entre las entradas y la salida de la misma manera que tuvimos muchas capas de convoluiciones cuando trabajando con imagens, las capas entre la entreda y la salida se llaman ocultas, el proceso de propagación hacia adelante sigue siendo el mismo que lo q ya has visto. En este caso calculo los vcalores de estas capas ocultas a partir de las capas entrada, se realiza un ligero tipo de calculo aritmetico para la capa de salida tmb se aplican funciones no lineales en cada nodo en las capas ocultas, incluida una no lineal, la funcion ayuda al modelo a capturar no linealidades e impactos de interaccion entre las variables mejor la funcion mas comun para aplicar es el rel o la funcion de activacion lineal rectificada, el tema es una tangente de lo que estamos enfocados en este momento.\n",
    "Los pesos cambiaran tus predicciones, por lo que los buenos pesos son la clave del bien.\n",
    "Los pesos son funciones de perdidia que crean un descenso y una propagacion hacia atrás, la funcion de perdida y el descenso de gradiente se muestra en nuestro codigo como argumento a la funcion de compilacion antes de ajustar nuestro modelo a la perdida. Los argumentos son valores reales del objetivo que tambien queriamos predecir como las predicciones d elos modelos como una convencion, utilizamos funciones de perdida donde las puntuaciones mas bajas son mejores, por lo que si los valores pronosticados estan cerca del valor real valores del objetivos, la funcion de perdida dara un valor bajo, si las predicciones generalmente están muy lejos de la funcion de perdida devolvera un valor alto.\n",
    "La clave de nuestro modelo es el procedimiento de optimizacion de descenso de gradiente y descenso de gradiente estocastico.\n",
    "Se suele lanzar un descenso de gradiente para establecer los pesos que minimizan nuestra perdida. Muchas personas describen el descenso de gradiente como una analogia los pesos afectan a nuestras predicciones y por lo tanto afectan la funcion de perdida.\n",
    "Descenso de gradiente: para encontrar el punto mas bajo, podras sentir en que direccion va cuesta abajo abruptamente y da un paso en esa direccion, entoncess sentiria alrededor de nuevo para encontrar una direccion que es cuata abajo y das otro paso, repites eso hasta que no puede bajar mas, esto basicamente es como actua el gradiente, el descenso funcion mira los datos,ve que puede cambiar la forma de obtener un poco baja la funcion de perdida y luego cambio el peso ligeramentee en esa direccion, repiter esto para mejorar el peso ligeramente de nuevo.\n",
    "\n",
    "La propagacion es el proceso por el cual descubrimos la forma en la que cambian los pesos\n",
    "**Loss=f(actual, predicted)**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
